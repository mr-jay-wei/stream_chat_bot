# chatbot_core/chatbot_pipeline.py

import asyncio
import time
import os
from typing import AsyncGenerator, Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor

from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—åŒ–ç»„ä»¶
from . import config
from .prompt_manager import prompt_manager
from .memory_manager import memory_manager
# from .hot_reload_manager import hot_reload_manager # å¯¼å…¥å…¨å±€ç®¡ç†å™¨å®ä¾‹

# å¯¼å…¥LangChainæ ¸å¿ƒç»„ä»¶
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# å¯¼å…¥æµå¼äº‹ä»¶å®šä¹‰
from dataclasses import dataclass
from enum import Enum

class StreamEventType(Enum):
    PROCESSING = "processing"
    GENERATION_START = "generation_start"
    GENERATION_CHUNK = "generation_chunk"
    GENERATION_END = "generation_end"
    ERROR = "error"
    COMPLETE = "complete"

@dataclass
class StreamEvent:
    type: StreamEventType
    data: Any
    timestamp: float

class ChatbotPipeline:
    """
    ä¼ä¸šçº§å¯¹è¯æœºå™¨äººæ ¸å¿ƒç®¡é“ (V1.1 - æ”¯æŒçƒ­é‡è½½å›è°ƒ)
    """
    def __init__(self):
        print("æ­£åœ¨åˆå§‹åŒ–ä¼ä¸šçº§å¯¹è¯æœºå™¨äºº...")
        self.executor = ThreadPoolExecutor(max_workers=os.cpu_count() or 4)
        self._setup_llm()
        
        # # âœ… å…³é”®æ”¹è¿›ï¼šå°†è‡ªå·±æ³¨å†Œä¸ºçƒ­é‡è½½çš„å›è°ƒæ¥æ”¶è€…
        # if hot_reload_manager and config.ENABLE_HOT_RELOAD:
        #     # self._on_prompt_reload å°±æ˜¯å›è°ƒå‡½æ•°ï¼Œå½“æ–‡ä»¶å˜åŒ–æ—¶å®ƒä¼šè¢«è°ƒç”¨
        #     hot_reload_manager.add_callback(self._on_prompt_reload)
        #     hot_reload_manager.start()
            
        print("ä¼ä¸šçº§å¯¹è¯æœºå™¨äººåˆå§‹åŒ–å®Œæˆã€‚")

    # âœ… æ–°å¢çš„å›è°ƒæ–¹æ³•
    def _on_prompt_reload(self, event_type: str, prompt_name: str):
        """
        å½“æç¤ºè¯æ–‡ä»¶è¢«çƒ­é‡è½½æ—¶ï¼Œæ­¤æ–¹æ³•ç”±HotReloadManagerè‡ªåŠ¨è°ƒç”¨ã€‚
        
        Args:
            event_type (str): äº‹ä»¶ç±»å‹ ('modified', 'created', 'deleted')
            prompt_name (str): è¢«æ”¹å˜çš„æç¤ºè¯åç§°
        """
        print(f"\nğŸ”¥ Pipelineæ”¶åˆ°çƒ­é‡è½½é€šçŸ¥! äº‹ä»¶: {event_type}, æç¤ºè¯: {prompt_name}")
        # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»»ä½•éœ€è¦æ›´æ–°çš„çŠ¶æ€ã€‚
        # ä¾‹å¦‚ï¼Œå¦‚æœæœªæ¥æˆ‘ä»¬æœ‰ä¸€äº›åŸºäºPromptæ„å»ºå¹¶ç¼“å­˜çš„å¤æ‚å¯¹è±¡ï¼Œ
        # å¯ä»¥åœ¨è¿™é‡Œæ¸…ç©ºæˆ–é‡å»ºå®ƒä»¬ã€‚
        # ç›®å‰ï¼Œæˆ‘ä»¬åªæ‰“å°ä¸€æ¡æ—¥å¿—æ¥è¯æ˜å›è°ƒé“¾è·¯å·²ç»æ‰“é€šã€‚
        print("âœ… PipelineçŠ¶æ€å·²åŒæ­¥ï¼ˆå½“å‰å®ç°æ— éœ€é¢å¤–æ“ä½œï¼‰ã€‚\n")

    def _setup_llm(self):
        api_key = os.getenv("DeepSeek_api_key")
        base_url = os.getenv("DeepSeek_base_url")
        model_name = os.getenv("DeepSeek_model_name")

        if not all([api_key, base_url, model_name]):
            raise ValueError("APIå¯†é’¥æˆ–æ¨¡å‹é…ç½®æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥.envæ–‡ä»¶ã€‚")
        
        print(f"  - é…ç½®å¤§è¯­è¨€æ¨¡å‹: {model_name}")
        self.llm = ChatOpenAI(
            model=model_name,
            openai_api_key=api_key,
            openai_api_base=base_url,
            temperature=0.7,
            streaming=True
        )
            
    async def _run_in_executor(self, func, *args):
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, func, *args)

    async def ask_stream(self, question: str, session_id: str = "default") -> AsyncGenerator[StreamEvent, None]:
        """
        æ ¸å¿ƒçš„æµå¼å¯¹è¯æ–¹æ³•
        """
        try:
            yield StreamEvent(type=StreamEventType.PROCESSING, data={"message": "æ€è€ƒä¸­..."}, timestamp=time.time())

            # æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°è·å–æœ€æ–°çš„æ¨¡æ¿ï¼Œç¡®ä¿çƒ­é‡è½½ç”Ÿæ•ˆ
            system_prompt_template = prompt_manager.get_template(config.SYSTEM_PROMPT_NAME)
            system_message_content = system_prompt_template.format()

            chat_history = []
            if config.ENABLE_SHORT_TERM_MEMORY:
                for turn in memory_manager.get_recent_conversations():
                    chat_history.append(HumanMessage(content=turn.question))
                    chat_history.append(AIMessage(content=turn.answer))

            messages = [SystemMessage(content=system_message_content)]
            messages.extend(chat_history)
            messages.append(HumanMessage(content=question))

            yield StreamEvent(type=StreamEventType.GENERATION_START, data={"message": "å¼€å§‹ç”Ÿæˆå›ç­”"}, timestamp=time.time())

            complete_answer = ""
            if hasattr(self.llm, 'astream'):
                async for chunk in self.llm.astream(messages):
                    chunk_content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                    if chunk_content:
                        complete_answer += chunk_content
                        yield StreamEvent(type=StreamEventType.GENERATION_CHUNK, data={"chunk": chunk_content}, timestamp=time.time())
            else:
                response = await self._run_in_executor(self.llm.invoke, messages)
                answer = response.content if hasattr(response, 'content') else str(response)
                complete_answer = answer.strip()
                for char in complete_answer:
                    yield StreamEvent(type=StreamEventType.GENERATION_CHUNK, data={"chunk": char}, timestamp=time.time())
                    await asyncio.sleep(0.02)

            if config.ENABLE_SHORT_TERM_MEMORY:
                memory_manager.add_conversation(question, complete_answer.strip())

            yield StreamEvent(type=StreamEventType.GENERATION_END, data={"message": "ç”Ÿæˆå®Œæˆ"}, timestamp=time.time())
            yield StreamEvent(type=StreamEventType.COMPLETE, data={"message": "å¯¹è¯å®Œæˆ"}, timestamp=time.time())

        except Exception as e:
            yield StreamEvent(type=StreamEventType.ERROR, data={"error": str(e)}, timestamp=time.time())
            
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œåœ¨å¯¹è±¡é”€æ¯æ—¶æ¸…ç†èµ„æºã€‚"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
            
        # # âœ… å…³é”®æ”¹è¿›ï¼šåœ¨å…³é—­æ—¶ï¼Œä¹Ÿä»ç®¡ç†å™¨ä¸­ç§»é™¤è‡ªå·±çš„å›è°ƒï¼Œå¹¶åœæ­¢ç›‘æ§
        # if hot_reload_manager:
        #     # æ£€æŸ¥å›è°ƒæ˜¯å¦å­˜åœ¨äºé›†åˆä¸­ï¼Œé¿å…KeyError
        #     if hasattr(self, '_on_prompt_reload') and self._on_prompt_reload in hot_reload_manager.callbacks:
        #          hot_reload_manager.remove_callback(self._on_prompt_reload)
        #     hot_reload_manager.stop()